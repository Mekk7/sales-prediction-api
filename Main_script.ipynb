{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Convert Calendar_Week to datetime\n",
    "data['Calendar_Week'] = pd.to_datetime(data['Calendar_Week'], errors='coerce')\n",
    "\n",
    "# Handle missing values (fill forward)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "# Plot Sales over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data['Calendar_Week'], data['Sales'], label='Sales', marker='o')\n",
    "plt.title('Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "correlation = data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "# Extract time-based features from Calendar_Week\n",
    "data['Month'] = data['Calendar_Week'].dt.month\n",
    "data['Week'] = data['Calendar_Week'].dt.isocalendar().week\n",
    "data['Day'] = data['Calendar_Week'].dt.day\n",
    "\n",
    "# Create impression ratio features\n",
    "data['Google_Impression_Ratio'] = data['Google_Impressions'] / data['Overall_Views']\n",
    "data['Facebook_Impression_Ratio'] = data['Facebook_Impressions'] / data['Overall_Views']\n",
    "data['Email_Impression_Ratio'] = data['Email_Impressions'] / data['Overall_Views']\n",
    "\n",
    "# Step 5: Define Features and Target\n",
    "X = data.drop(columns=['Sales', 'Calendar_Week', 'Division'])  # Features (drop non-numeric columns)\n",
    "y = data['Sales']  # Target variable (Sales)\n",
    "\n",
    "# Print feature names to see the order and names\n",
    "print(\"Feature names used for training:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# Step 6: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert 'Week' column to int64\n",
    "X_train['Week'] = X_train['Week'].astype(np.int64)\n",
    "X_test['Week'] = X_test['Week'].astype(np.int64)\n",
    "\n",
    "# Check the data types in X_train\n",
    "print(\"Data types in X_train:\")\n",
    "print(X_train.dtypes)\n",
    "\n",
    "\n",
    "# Step 7: Model Training (Linear Regression and Random Forest)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Model Prediction\n",
    "# Predict using both models on the test set\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Step 9: Model Evaluation (Linear Regression vs Random Forest)\n",
    "# Linear Regression\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = mean_squared_error(y_test, y_pred_lr, squared=False)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "print(f\"Linear Regression - MAE: {mae_lr}, RMSE: {rmse_lr}, R2: {r2_lr}\")\n",
    "\n",
    "# Random Forest\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest - MAE: {mae_rf}, RMSE: {rmse_rf}, R2: {r2_rf}\")\n",
    "\n",
    "# Step 10: Feature Importance from Random Forest Model\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "# Step 11: Hyperparameter Tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "}\n",
    "\n",
    "# Grid Search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 12: Evaluate Best Model\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "mae_best_rf = mean_absolute_error(y_test, y_pred_best_rf)\n",
    "rmse_best_rf = mean_squared_error(y_test, y_pred_best_rf, squared=False)\n",
    "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
    "\n",
    "print(f\"Best Random Forest Model - MAE: {mae_best_rf}, RMSE: {rmse_best_rf}, R2: {r2_best_rf}\")\n",
    "\n",
    "# Step 13: Save the Best Model for Future Use\n",
    "joblib.dump(best_rf_model, 'sales_prediction_model.pkl')\n",
    "\n",
    "# Load the model back for predictions (optional)\n",
    "loaded_model = joblib.load('sales_prediction_model.pkl')\n",
    "y_pred_loaded_model = loaded_model.predict(X_test)\n",
    "\n",
    "# Verify the predictions from the saved model\n",
    "print(f\"Loaded Model - First few predictions: {y_pred_loaded_model[:5]}\")\n",
    "\n",
    "import shap\n",
    "\n",
    "# Step 3: Explain the model's predictions using SHAP\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer(best_rf_model, X_train)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Step 4: Visualize the SHAP values\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Feature importance plot for a specific prediction\n",
    "# Choose the first instance in the test set for explanation\n",
    "shap.initjs()\n",
    "shap.plots.waterfall(shap_values[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
